<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Detection of Fast-Moving Objects with Neuromorphic Hardware">
  <meta name="keywords" content="Table Tennis Robot, Event Camera, Spiking Neural Networks, Event-Based Computer Vision, Object Detection">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>An Event-Based Perception Pipeline for a Table Tennis Robot</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Detection of Fast-Moving Objects with Neuromorphic Hardware</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://andreasaziegler.github.io/">Andreas Ziegler</a><sup>1</sup>,</span>
            <span class="author-block">
              Karl Vetter</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://uni-tuebingen.de/de/226101">Thomas Gossard</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://uni-tuebingen.de/de/138688">Jonas Tebbe</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://adaptiveailab.github.io/team/sebastian/">Sebastian Otte</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://uni-tuebingen.de/de/138703">Andreas Zell</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Tübingen,</span>
            <span class="author-block"><sup>2</sup>University of Lübeck</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2403.10677"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.10677"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://youtu.be/uLgIhVKnsHg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cogsys-tuebingen/snn-edge-benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/cogsys-tuebingen/snn-edge-benchmark/tree/main/dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center>
        <img class="round" style="width:600px" src="./static/images/teaser.png" />
      </center>
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
      <!--   <source src="./static/videos/teaser.mp4" -->
      <!--           type="video/mp4"> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        Left: Three examples of 2D ball detections in an accumulated event frame which serves as the input to the Spiking Neural Network (SNN) with <font color="green">ground truth in green</font> and the <font color="red">estimated position in red</font>. Right: Five observed 2D trajectories in the camera frame of the event-based camera with <font color="green">ground truth in green</font> and the <font color="red">estimated positions in red</font>. Background: The table tennis robot setup with the robot hitting back a table tennis ball in a rally.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neuromorphic Computing (NC) and Spiking Neural Networks (SNNs) in particular are often viewed as the next generation of Neural Networks (NNs). NC is a novel bio-inspired paradigm for energy efficient neural computation, often relying on SNNs in which neurons communicate via spikes in a sparse, event-based manner. This communication via spikes can be exploited by neuromorphic hardware implementations very effectively and results in a drastic reductions of power consumption and latency in contrast to regular GPU-based NNs. In recent years, neuromorphic hardware has become more accessible, and the support of learning frameworks has improved. However, available hardware is partially still experimental, and it is not transparent what these solutions are effectively capable of, how they integrate into real-world robotics applications, and how they realistically benefit energy efficiency and latency. In this work, we provide the robotics research community with an overview of what is possible with SNNs on neuromorphic hardware focusing on real-time processing. We introduce a benchmark of three popular neuromorphic hardware devices for the task of event-based object detection. Moreover, we show that an SNN on a neuromorphic hardware is able to run in a challenging table tennis robot setup in real-time.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/uLgIhVKnsHg?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{Ziegler2025icra,
  title={Detection of Fast-Moving Objects with Neuromorphic Hardware},
  url = {https://arxiv.org/pdf/2403.10677},
  author={Ziegler,  Andreas and Vetter,  Karl and Gossard,  Thomas and Tebbe,  Jonas and Otte,  Sebastian and Zell, Andreas},
  booktitle = {2025 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  publisher = {IEEE},
  year={2025},
}</code></pre>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Acknowledgements. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgements</h2>
        <div class="content has-text-justified">
          <p>
            This research was partially funded by <a href="https://ai.sony">Sony AI</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Acknowledgements. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io"><span class="dnerf">Nerfies</span></a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
